\chapter{Problem Setting}
\label{chap:problem}
During scientific expeditions, the objective of a robot is to collect useful measurements, as defined by a given task (e.g., reduce uncertainty over a quantity, find the global optimum in a distribution, track a moving target). For hydrothermal plume charting, the goal is to map or ``chart'' the spatiotemporal structure of a buoyant plume using a dynamically constrained AUV. Such a chart enables scientists to infer relevant scientific properties of generating vents (e.g., chemical flux) and to create detailed models of deep-sea interactions and nutrient cycling. We first describe how general problems in expeditionary science can be formulated as sequential decision-making problems, then describe the specific constraints of the AUV \Sentry and how they influence the sequential decision-making problem, and finally we present a formal description of hydrothermal charting as a partially observable Markov decision process. 

\section{Scientific Expeditions as a Sequential Decision-Making Problem}
Expeditionary science requires a robot to make a sequence of decisions to collect scientifically useful measurements of an unknown, partially-observable spatiotemporal environment under operational constraints. Generally, the expeditionary science decision-making problem can be formulated as a partially observable Markov decision-process (POMDP). Let $\Pi(\cdot)$ denote the space of probability distributions over the argument. A finite horizon POMDP can be represented as tuple: $(\Ss, \A, T, R, \Zz, O, b_0, H, \gamma)$, where $\Ss$ are the states, $\A$ are the actions, and $\Zz$ are the observations. At planning iteration $t$, the agent selects an action $a \in \A$ and the transition function $T: \Ss \times \A \to \Pi(\Ss)$ defines the probability of transitioning between states in the world, given the current state $s$ and control action $a$. The transition function governs both how the state of the robot will evolve, given a chosen action, and the potentially stochastic evolution of the underlying spatiotemporal environment. After the state transition, the agent receives an observation according to the observation function $O: \Ss \times \A \to \Pi(\Zz)$, which defines the probability of receiving an observation, given the current state $s$ and previous control action $a$. The reward function $R: \Ss \times \A \to \reals$ serves as a specification of the task, assigning the states of the world that are useful for a given scientific objective high reward and others low reward. A POMDP is initialized with belief $b_0 \in \Pi(\Ss)$ --- an initial probability distribution over state --- and plans over horizon $H \in \integers^+$ with discount factor $\gamma \in [0, 1]$.

As the robot moves through the world, it selects actions and receives observations. Since the state of the world is not directly observable in a POMDP, the robot maintains a probability distribution over possible states (i.e., belief) and must update this distribution each time it takes an action and receives an observation. Given the transition and observation models, the belief can be updated directly using a Bayes filter \autocite{sarkka2013bayesian}:
\begin{align}
    \tau(b_{t-1}, a_{t-1}, z_t) = b_t
        &\defeq \Pi(S_t \mid a_0, z_0, \dots, a_{t-1}, z_{t-1}, z_t) \\
        &= \Pi(S_t \mid b_{t-1}, a_{t-1}, z_t) \\
        &= \frac{\int_{s \in \Ss} O(s, a_{t-1}, z_t) T(s, a_{t-1}, s')b_{t-1}(s')}{\Pi(z_t \mid b_{t-1}, a_{t-1})}
    \label{eq:bayes_tau}
\end{align}

where $\tau(b,a,z)$ is the updated belief after taking control action $a$ and receiving observation $z$ (Eq.~\ref{eq:bayes_tau}). Unfortunately, \cref{eq:bayes_tau} is often intractable to compute directly and an approximate Bayesian inference procedure is required to represent the belief (e.g., a Kalman filter \autocite{welch1995introduction}, a particle filter \autocite{Silver2010}, or variational methods \autocite{wainwright2002environmental,kucukelbir2017automatic}). 

Due to the stochastic, partially observable nature of current and future states, the realized reward in a POMDP is a random variable. Optimal planning is defined as finding a horizon-dependent policy $\{\pi_t^*: \Pi(\Ss) \to \A\}_{t=0}^{H-1}$ that maximizes expected reward: $\mathbb{E} \Big[ \sum_{t=0}^{H-1} \gamma^t R\big(S_t, \pi_t(b_t)\big) \mid b_0 \Big]$, where $b_t$ is the updated belief at time $t$, conditioned on the history of actions and observations. The recursively defined horizon-$h$ optimal value function $V^*_h$ quantifies, for any belief $b$, the expected cumulative reward of following an optimal policy over the remaining planning iterations: $V_0^{*}(b) = \truemax_{a \in \A} \mathbb{E}_{s \sim b}[R(s, a)]$ and
\begin{align}
     V_h^{*}(b) &=  \max_{a \in \A} \mathbb{E}_{s \sim b}[R(s, a)] + \gamma \int_{z \in \Zz} \Pi(z \mid b, a) V_{h-1}^*(\tau(b, a,z)) \, \text{d}z \hspace{0.6cm} h \in [1, H-1],
    \label{eq:value}
\end{align}
The optimal policy at horizon $h$ is to act greedily according to a one-step look ahead of the horizon-$h$ value function. However, \cref{eq:value} is  intractable for large or continuous state, action, or observation spaces and thus the optimal policy must be approximated. Much of the art of practical decision-making uncertainty is making well-designed algorithmic and heuristic choices that enable efficient and robust planning algorithms. In the remainder of this section, we introduce the plume charting POMDP with AUV \Sentry and in \cref{sec:methods}, we describe the specific algorithmic choices that enable \PHORTEX to approximately solve it.  


\section{Scientific Decision-Making with AUV \emph{Sentry}}
AUV \emph{Sentry} is capable of autonomously navigating between given waypoints using a closed-loop controller and a state estimator that uses acoustic ranging between the robot and the ship to set latitude, longitude, and depth coordinates. At present, \emph{Sentry} is not capable of \emph{underway} or online decision-making, in which waypoints are adaptively set on-the-fly while the robot is executing its mission. The lack of underway abilities is both a logistical and policy obstacle. Logistically, the robot is computationally limited and solving a POMDP online often requires significant onboard compute. \Sentry relies on a high-latency acoustic link to communicate with the ship, meaning that data from \emph{Sentry} cannot be streamed to an external computing resource on the ship for decision making (science data communication between ship and robot is 0.02 Hz assuming no packet loss, and only a subset of sensor data can be made available in any given packet). Additionally, by policy, \Sentry trajectories are rigorously vetted before each dive using bathymetric maps of the target region and dynamics validation schemes. Extreme (and warranted) risk aversion to avoid losing or damaging \Sentry leads to the policy that underway plan changes cannot be part of normal operating procedures.

Thus, to enable sequential decision-making with \Sentry requires the use of \emph{deployment-by-deployment} autonomy. Unlike underway decision-making, deployment-by-deployment autonomy does not modify the AUV trajectory in real-time, but instead leverages the ``down-time'' between robot deployments to post-process data, update a belief model about the environment, and plan a new fixed trajectory for the next deployment to execute. This form of autonomy honors the strong requirement that each deployment must pass through a rigorous safety and validation check, while enabling adaptive search behavior based on accrued knowledge between deployments. Each planning ``step'' or iteration in the POMDP framework is an entire deployment of \Sentry. In the following section, the implications of this constraint are codified within a POMDP framework.

% \subsection{Types of Closed-loop Autonomy}
% There are several levels at which an AUV can behave autonomously. At the lowest level, the AUV is given navigation waypoints and executes a closed-loop controller and state estimator to drive to that waypoint; this type of autonomy is commonly implemented and executed on AUV platforms. More sophisticated autonomy systems that are aware of scientific objectives can build upon these waypoint controllers to enable sophisticated autonomous behavior.

% One level up is \textit{underway} autonomy: after a waypoint is reached, an autonomy stack chooses the next waypoint for a vehicle to target. When operating with underway autonomy, the AUV can act in closed-loop, using observations of the environment in real-time to inform subsequent waypoints while underway. Underway autonomy has the potential to greatly increase the utility of the collected data. For example, the AUV may serendipitously encounter plume water while navigating to a waypoint; an underway autonomy system could then attempt to follow chemical gradients to the plume center. However, there are downsides to underway autonomy, both practical and due to human factors. First, underway autonomy requires that both model updates and planning are run on the robot itself. This quickly leads to computational challenges when models are represented by expensive dynamical simulators and planners necessitate expensive state rollouts. Second, while an AUV is underway, scientists are only able to communicate with the vehicle over low-bandwidth, high latency acoustic links. This leads to serious safety concerns; if the vehicle makes a poor decision and sets an unsafe waypoint, it can take several minutes for that information to be communicated with the ship and for mediating actions to be taken. Therefore, there is significant hesitancy from scientists and engineers alike to enable underway autonomy and the bar to show that an algorithm is safe and trustworthy is incredibly high.

% The focus of this paper will instead be on \textit{deployment-by-deployment} autonomy. Unlike underway autonomy, deployment-by-deployment autonomy does not modify the AUV trajectory in real-time while it is underway. In most robotic science missions, an AUV will be deployed in a sequence of dives. While underway on a single dive, the AUV will be constrained to execute an open-loop trajectory that was chosen before the dive. This pre-planned trajectory can undergo a rigorous set of safety checks before vehicle deployment and scientists can examine and validate the planned trajectories. Then, between dives, the data collected on the previous dive can be used to update a model of the environment and inform the subsequent dive. Model updates and planning can be done with high-power computers on a ship or field station; the robot itself only needs to execute the planned trajectory. Although deployment-by-deployment autonomy is less flexible and reactive than underway autonomy, it is a very useful and practical form of autonomy for many applications of scientific robots.

% This closed-loop, deployment-by-deployment decision-making is the focus of the remainder of the paper and our field deployment.

\section{Charting Hydrothermalism as a POMDP}
\label{sec:pomdp}
The plume charting POMDP can be formalized as follows: 

\paragraph{The state space $\Ss$} The state space of the plume-charting POMDP consists of the joint continuous states of the environment (i.e., the plume) and the robot. The environment state will be represented by a $d$-dimensional vector of continuous plume parameters $\x_p \in \R^d$ and a current vector $\x_c \in \R^2$ that contains the heading and velocity of the prevailing crossflow, which vary in time. The robot state will be represented by a vector $\x_r \in \R^3$ that represents the latitude, longitude, and depth of the robot.

\paragraph{The action space $\A$} The action space of the plume-charting POMDP consists of sequences of parameterized lawnmower pattern (i.e., back-and-forth uniform coverage) trajectory primitives. The selection of the lawnmower as the base primitive was given by \Sentry operators. By chaining lawnmower trajectories together during a deployment, a relatively expressive action set is available. Each trajectory primitive is parameterized by a set of $b$ real-valued parameters $\theta \in \Theta \subseteq \R^b$. These parameters include scale (height and width that describe the rectangle in which the lawnmower is contained), resolution (the absolute distance between tracklines of the lawnmower), and global position (latitude-longitude-depth coordinate and planar angle of the origin of the primitive). The robot's action set then consists of sequences of parameterized trajectories, i.e., $\A = \Theta^n, \, n \in \Z^+$. The number of trajectory objects $n$ and the altitude or depth for which a trajectory will be executed for a given chain is fixed \emph{a priori} to planning. 

\paragraph{The transition function $T$} The transition function $T(s, a, s')$ will be decomposed into a plume transition $T_p$, a current transition $T_c$, and a robot transition function $T_r$.
\begin{itemize}
	\item The plume state parameters $\x_p$, e.g., venting characteristics like plume exit velocity or vent temperature, are assumed to be constant and therefore the plume transition function $T_p$ is given by: $T_p(\x_p, a, \x_p') = \delta_{\x_p = \x_p'} \, \forall a \in \A, \x_p, \x_p' \in \R^d$. Although it is possible for plume parameters to vary on a timescale relevant to a robotic deployment (over the course of hours \autocite{chevaldonne1991time}), the overall impact to gross features of plume rise height, bend angle, and cross-sectional area is essentially negligible, which is reflected in the form of the transition function provided.
	\item The current transition function $T_c$ is more complex and driven by tidal cycles, local bathymetry, and deep sea currents. We will estimate a deterministic current transition function $T_c(\x_c, a, \x_c') = \delta_{\x_c' = h(\x_c)} \, \forall a \in \A, \x_c, \x_c' \in \R^2$, where the function $h$ evaluates the future current magnitude and heading from the present current state, from point observations of current magnitude and heading from a sensor that is not part of the robot (described in detail in \cref{sec:external_current}). 
	\item The robot transition function $T_r$ assumes that the robot's waypoint controller is deterministically able to execute a planned trajectory: $T_r(\x_r, a, \x_r') = \delta_{\x_r' = g(\x_r, a)}$, where the function $g$ evaluates the goal waypoint of the trajectory given by $a$. Although there is some uncertainty in the robots transition, in practice in our field application, localization and control were well-solved problems and pose uncertainty contributed minimally to the robot's task execution compared with uncertainty about the plume state.
\end{itemize}


\paragraph{The reward function $R$} The reward function for the plume-charting POMDP encodes the robot's objective to produce a comprehensive map of the plume. We choose to approximate this objective by rewarding the robot for collecting observations of ``plume fluids'', i.e., water that is expected to be derived from hydrothermal vents as indicated by our belief of the environmental state $R([\x_p, \x_c, \x_r]^\top, a) = \indic{\texttt{in\_plume}(\x_p, \x_c, \x_r, a)}$. 
%This reward function encourages the robot to align it's lawnmower trajectories with the the plume envelope and placing constraints on the minimal size of the a lawnmower trajectory ensured that a diversity of in-plume and out-of-plume samples were collected. Although this is greatly simplified heuristic reward function, it comparatively inexpensive to compute compared to some information theoretic rewards, that would more directly measure uncertainty reduction in, e.g., plume parameters, and worked well in practice. Further, because of the severe constraint requiring the use of lawnmower primitives, the use of this reward function does not severely reduce the collection of notionally ``diverse'' samples of plume expression which might be uncertain according to the model.

\paragraph{The observation space $\Zz$} The robot carries a variety of scientific and navigational sensors. We use a sensor model that fuses and converts complex, continuous scientific observations into a simplified measurement of plume content in a given fluid parcel $z_p \in \{0, 1\}$, discussed in \cref{sec:sensor_models}. By performing this filtering step, we significantly reduce the dimensionality and complexity of the observation space. Outside of the robot, a sensing system provides independent observations of current magnitude $z_g \in \mathbb{R}^+$ and heading $z_h \in \{(-180, 180]\}$. Thus the observation space $\Zz$ consists of multiple observations of $z_p$, $z_g$, and $z_h$.

\paragraph{The measurement function $O$} The measurement function encodes the relationship between the plume parameters and heterogeneous scientific sensors on the robot, the prevailing current with the external sensing system deployed by the science party, and the robot location with the navigation equipment aboard the vehicle and ship. We make use of a sensor model described in \cref{sec:sensor_models} to process scientific sensor data into a measurement that indicates whether a fluid parcel was derived by a plume, and utilize \PHUMES (\cref{sec:phumes}) to map both current data and the simplified plume measurement to plume parameters. We assume that the robot position is fully-observable and exactly reported by the navigation equipment. 

% The measurement function encodes the complex relationship between the plume parameters, prevailing current, and robot state with the heterogeneous scientific sensors on the robot, each with their own complex sensor physics and external sensing equipment. Our measurement function consists of two pieces: 1) an analytical plume model that maps plume parameters $\x_p$ and crossflow state $\x_c$ to the distribution of plume fluids in a 3D volume over time (see \cref{sec:phumes}), and 2) a binary pseudo-sensor that fuses signals from the heterogeneous sensing payload into binary plume detections that can be directly compared to the output of the analytical plume model. %We add a simple Bernoulli \td{check} noise model to the pseudo-sensor measurement with an assumed false positive/false negative rate.

\paragraph{The horizon $H$ and discount factor $\gamma$} In deployment-by-deployment autonomy, the horizon $H$ can be set to be equal to the total number of deployments to be conducted during an expedition and the discount factor $\gamma$ set to $1.0$. However, practically, the state of \Sentry at the end of one deployment often has little or no impact on its achievable reward in the subsequent deployment due to the constrained nature of deployments and the ability for \Sentry to be released or recovered from a ship at arbitrary coordinates. Under this assumption, the discount factor can be set to zero $\gamma=0$ to break the finite-horizon sequential decision making problem into a sequence of horizon-$1$ planning problems. This reduces the capacity of the planner to reason about long-term, multi-dive information gathering actions, but, as we see in the following sections, computationally simplifies the planning problem.

% However, an important approximation we make in our specific deployment-by-deployment autonomy problem is that the state of the robot at the end of Dive 1 has little or no impact on its achievable reward in Dive 2 and subsequent dives. This assumption has the impact of setting $\gamma=0$ and breaks the finite-horizon sequential decision making problem into a sequence of horizon-$1$ planning problems. The consequences of this assumption may or may not be reasonable in applications of deployment-by-deployment autonomy. It reduces the robots capacity to reason about long-term, multi-dive information gathering actions, for example. However, there are significant computational benefits of performing only single-stage planning and in our hydrothermal plume application, the assumption  was reasonable, as the robot was constrained to return to the stationary ship at the end of each dive for recovery and recharging.
% The plume transition function will be represented by a deterministic analytical model $\f_t(\x_p)$ (see \td{MTT model section})

